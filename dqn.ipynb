{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Deep Q-Learning (DQN)</h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN uses a neural network to approximate the Q-values. The loss function such as the MSE loss can be:\n",
    "$$ L = \\left(Q(s, a;\\theta) - (r + \\gamma \\max_{a'} Q(s', a';\\theta)) \\right) ^ 2 \\tag{3}$$\n",
    "\n",
    "The update rule depends on the values produced by the network itself, making convergence diffucult. To address this, the DQN algorithm introduces the use of a replay buffer and target networks. The replay buffer stores past interactions as a list of tuples, which can be sampled to update the value and policy networks. This allows the network to learn from individual tuples multiple times and reduces dependence on the current experience. The target networks are time-delayed copies of the policy and Q-networks, and their parameters are updated according to the following equations:\n",
    "\n",
    "$$ \\theta_Q' \\leftarrow \\tau \\theta_Q + (1 - \\tau) \\theta_Q' \\tag{4}$$\n",
    "$$ \\theta_\\mu' \\leftarrow \\tau \\theta_\\mu + (1 - \\tau) \\theta_\\mu' \\tag{5}$$\n",
    "\n",
    "where $\\theta_\\mu'$ and $\\theta_Q'$ denote the parameters of the policy and Q-networks, respectively.\n",
    "\n",
    "![](./images/dqn.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prelims "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "GAMMA = 0.99\n",
    "MINI_BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 50000\n",
    "MIN_REPLAY_SIZE = 1000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.02\n",
    "EPSILON_DECAY = 10000\n",
    "TARGET_UPDATE_FREQUENCY = 1000\n",
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dqn import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_step(\n",
    "    env: gym.Env,\n",
    "    observation: np.ndarray,\n",
    "    online_net: Network,\n",
    "    target_net: Network,\n",
    "    epsilon_gen,\n",
    "    optimizer,\n",
    "    replay_buffer: deque,\n",
    "    reward_buffer: deque,\n",
    "    episode_reward: float,\n",
    "    criterion: function,\n",
    "    batch_size: int,\n",
    "):\n",
    "    epsilon = next(epsilon_gen)\n",
    "    random_sample = random.random()\n",
    "    action = (\n",
    "        env.action_space.sample()\n",
    "        if random_sample <= epsilon\n",
    "        else online_net.act(observation)\n",
    "    )\n",
    "\n",
    "    next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "    replay_buffer.append((observation, action, reward, terminated, next_observation))\n",
    "    observation = next_observation\n",
    "\n",
    "    episode_reward += reward\n",
    "\n",
    "    if terminated:\n",
    "        observation, info = env.reset()\n",
    "        reward_buffer.append(episode_reward)\n",
    "        episode_reward = 0.0\n",
    "\n",
    "    # Start Gradient Step\n",
    "    memories = random.sample(replay_buffer, batch_size)\n",
    "    all_obs_tensor = torch.as_tensor(\n",
    "        [memo_i[0] for memo_i in memories], dtype=torch.float32\n",
    "    )  # [batch, 4]\n",
    "    all_a_tensor = torch.as_tensor(\n",
    "        [memo_i[1] for memo_i in memories], dtype=torch.int64\n",
    "    ).unsqueeze(\n",
    "        -1\n",
    "    )  # [batch, 1]\n",
    "    all_r_tensor = torch.as_tensor(\n",
    "        [memo_i[2] for memo_i in memories], dtype=torch.float32\n",
    "    ).unsqueeze(\n",
    "        -1\n",
    "    )  # [batch, 1]\n",
    "    all_done_tensor = torch.as_tensor(\n",
    "        [memo_i[3] for memo_i in memories], dtype=torch.float32\n",
    "    ).unsqueeze(\n",
    "        -1\n",
    "    )  # [batch, 1]\n",
    "    all_next_obs_tensor = torch.as_tensor(\n",
    "        [memo_i[4] for memo_i in memories], dtype=torch.float32\n",
    "    )  # [batch, 4]\n",
    "\n",
    "    # Compute Targets\n",
    "    target_q_values = target_net(all_next_obs_tensor)  # [batch, 2]\n",
    "    max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]  # [batch, 1]\n",
    "    targets = all_r_tensor + GAMMA * (1 - all_done_tensor) * max_target_q_values\n",
    "\n",
    "    # Compute Loss\n",
    "    q_values = online_net(all_obs_tensor)\n",
    "    a_q_values = torch.gather(input=q_values, dim=1, index=all_a_tensor)\n",
    "    loss = criterion(a_q_values, targets)\n",
    "\n",
    "    # Gradient Descent\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return observation, episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_replay_buffer(replay_buffer, env: gym.Env, buffer_size: int):\n",
    "    observation, info = env.reset()\n",
    "    for _ in range(buffer_size):\n",
    "        action = env.action_space.sample()\n",
    "        observation_, reward, terminated, truncated, info = env.step(action)\n",
    "        memory = (observation, action, reward, terminated, observation_)\n",
    "        replay_buffer.append(memory)\n",
    "        observation = observation_\n",
    "\n",
    "        if terminated:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_generator(start, end, decay):\n",
    "    step = 0\n",
    "    while True:\n",
    "        epsilon = np.interp(step, [0, decay], [start, end])\n",
    "        yield epsilon\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "reward_buffer = deque([0.0], maxlen=100)\n",
    "episode_reward = 0.0\n",
    "\n",
    "online_net = Network(env)\n",
    "target_net = Network(env)\n",
    "optimizer = torch.optim.Adam(online_net.parameters(), lr=5e-4)\n",
    "criterion = nn.functional.smooth_l1_loss\n",
    "\n",
    "create_replay_buffer(replay_buffer, env, BUFFER_SIZE)\n",
    "epsilon_gen = epsilon_generator(EPSILON_START, EPSILON_END, EPSILON_DECAY)\n",
    "observation, info = env.reset()\n",
    "\n",
    "for step in range(20):\n",
    "    online_net.train()\n",
    "    observation, episode_reward = run_step(\n",
    "        env=env,\n",
    "        observation=observation,\n",
    "        online_net=online_net,\n",
    "        target_net=target_net,\n",
    "        epsilon_gen=epsilon_gen,\n",
    "        optimizer=optimizer,\n",
    "        replay_buffer=replay_buffer,\n",
    "        reward_buffer=reward_buffer,\n",
    "        episode_reward=episode_reward,\n",
    "        criterion=criterion,\n",
    "        batch_size=MINI_BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    # # After solved, watch it play\n",
    "    # if len(reward_buffer) >= 100:\n",
    "    #     if np.mean(reward_buffer) >= 30000:\n",
    "    #         while True:\n",
    "    #             action = online_net.act(observation)\n",
    "    #             observation, reward, terminated, truncated, info = env.step(action)\n",
    "    #             env.render()\n",
    "\n",
    "    #             if terminated:\n",
    "    #                 env.reset()\n",
    "\n",
    "    # Update target network\n",
    "    if step % TARGET_UPDATE_FREQUENCY == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "    # Print the training progress\n",
    "    if step % 1000 == 0:\n",
    "        print()\n",
    "        print(\"Step: {}\".format(step))\n",
    "        print(\"Avg reward: {}\".format(np.mean(reward_buffer)))\n",
    "        torch.save(online_net.state_dict(), \"dqn_model_{}.pth\".format(step))\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
