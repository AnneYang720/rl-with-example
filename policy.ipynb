{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Vanilla Policy Gradient Algorithm</h1></center>\n",
    "\n",
    "[For detail](https://medium.com/analytics-vidhya/a-deep-dive-into-vanilla-policy-gradients-3a79a95f3334)\n",
    "\n",
    "[zhihu](https://zhuanlan.zhihu.com/p/599897265)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Concepts\n",
    "\n",
    "## Reward Functions\n",
    "\n",
    "### Finite-horizon undiscounted return\n",
    "It sums up the rewards from a set of actions.\n",
    "$$ R(\\tau) = \\sum_{t=0}^{T} r_t $$\n",
    "\n",
    "### Infinite-horizon discounted return\n",
    "It has a new coefficient referred to as the discount factor. The discount factor corresponds to how far in the future those rewards were collected and is a number between zero and one.\n",
    "$$ R(\\tau) = \\sum_{t=0}^{\\infty} \\gamma^t r_t $$\n",
    "\n",
    "## Value Functions\n",
    "$$ V^{\\pi}(s) = E_{\\tau \\sim \\pi} [R(\\tau) | s_0 = s] $$\n",
    "$$ Q^{\\pi}(s, a) = E_{\\tau \\sim \\pi} [R(\\tau) | s_0 = s, a_0 = a] $$\n",
    "\n",
    "## Advantange Function\n",
    "$$ A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Policy Gradient Algorithm\n",
    "\n",
    "## Reward-to-go Method\n",
    "Only rewards after action can have an effect on the future actions, which is called the reward-to-go:\n",
    "$$ \\hat R_t = \\sum_{t'=t}^{T} R(s_{t'},a_{t'},s_{t'+1}) $$ \n",
    "\n",
    "$$ \\nabla J(\\pi_\\theta) = E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla \\theta \\log \\pi_\\theta(a_t|s_t) \\sum_{t'=t}^{T} R(s_{t'},a_{t'},s_{t'+1}) \\right] $$\n",
    "\n",
    "## Advantage Function\n",
    "The gradient of advantage function is:\n",
    "$$ \\nabla J(\\pi_\\theta) = E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla \\theta \\log \\pi_\\theta(a_t|s_t) A_\\pi(s_t, a_t) \\right]$$\n",
    "\n",
    "The value function $V_\\pi(s_t)$ is estimated by an approximate value function $V_{\\phi}(s_t)$, which is updated by:\n",
    "$$ \\phi_k = argmin_{\\phi} E_{s_t, \\hat R_t \\sim \\pi_k} [(V_{\\phi}(s_t) - \\hat R(t))^2] $$\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/vpg_algo.webp\" alt=\"example\">\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
