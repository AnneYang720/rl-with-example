{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(1, -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class Baseline(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        super(Baseline, self).__init__()\n",
    "        shape = 1\n",
    "        for dim in input_shape:\n",
    "            shape *= dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(shape, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(1, -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, policy_net, baseline_net):\n",
    "        self.policy_net = policy_net\n",
    "        self.baseline = baseline_net\n",
    "\n",
    "    def train(self, env, num_traj, iterations, gamma, base_epochs):\n",
    "        iter_rewards = []\n",
    "        for iter in range(iterations):\n",
    "            trajectories = []\n",
    "            ITER_REW = 0\n",
    "            for _ in range(num_traj):\n",
    "                rewards = []\n",
    "                log_probs = []\n",
    "                s,_ = env.reset()\n",
    "                done = False\n",
    "                while not done:\n",
    "                    s = torch.FloatTensor([s]).cuda()\n",
    "                    a = self.policy_net(s)\n",
    "                    del s\n",
    "                    a2 = a.detach().cpu().numpy()\n",
    "                    vec = [0, 1]\n",
    "                    u = np.random.choice(vec, 1, replace=False, p=a2[0])\n",
    "                    log_probs.append(a[0][u])\n",
    "                    del a\n",
    "                    sp, r, done, _, _ = env.step(u[0])\n",
    "                    # if done:\n",
    "                    #     if len(rewards) < 50:\n",
    "                    #         r = -200\n",
    "                    ITER_REW += r\n",
    "                    rewards.append(r)\n",
    "                    # env.render()\n",
    "                    s = sp\n",
    "                trajectories.append({'log_probs': log_probs, 'rewards': rewards})\n",
    "            # self.update_baseline(base_epochs, trajectories, gamma)\n",
    "            self.update_policy(trajectories, gamma)\n",
    "            print(\"ITERATION:\", iter+1, \"AVG REWARD:\", ITER_REW/num_traj)\n",
    "            iter_rewards.append(ITER_REW/num_traj)\n",
    "        return iter_rewards\n",
    "\n",
    "    def update_baseline(self, epochs, trajectories, gamma):\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optim = torch.optim.Adam(self.baseline.parameters())\n",
    "        for epoch in range(epochs):\n",
    "            loss = torch.tensor(0).float().cuda()\n",
    "            for trajectory in trajectories:\n",
    "                for t in range(len(trajectory)):\n",
    "                    r_t = 0\n",
    "                    for t_d in range(t, len(trajectory)):\n",
    "                        r_t += gamma**(t_d - t) * trajectory[t_d]['r']\n",
    "                    pred = self.baseline(trajectory[t]['s'])\n",
    "                    loss += criterion(pred, torch.FloatTensor([r_t]).cuda())\n",
    "            print(loss.item())\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    def update_policy(self, trajectories, gamma):\n",
    "        loss = torch.tensor([0]).float().cuda()\n",
    "        optim = torch.optim.Adam(self.policy_net.parameters(), lr=0.1)\n",
    "        for trajectory in trajectories:\n",
    "            for t in range(len(trajectory['rewards'])):\n",
    "                r_t = 0\n",
    "                log_prob = trajectory['log_probs'][t]\n",
    "                temp = trajectory['rewards'][t:]\n",
    "                for i, reward in enumerate(temp):\n",
    "                    r_t += gamma**i * reward\n",
    "                # for t_d in range(t, len(trajectory)):\n",
    "                    # r_t += gamma ** (t_d - t) * trajectory['rewards'][t_d]\n",
    "                # print(trajectory[t]['s'])\n",
    "                # advantage = torch.FloatTensor([r_t]).cuda() - self.baseline(trajectory[t]['s'])[0]\n",
    "                advantage = torch.FloatTensor([r_t]).cuda()\n",
    "                loss += -log_prob * advantage\n",
    "                # loss.backward()\n",
    "            # loss += -log_probs * advantage\n",
    "        loss = loss/len(trajectories)\n",
    "        loss.backward()\n",
    "        # print(\"\\nBefore zerograd\\n\")\n",
    "        # for name, param in self.policy_net.named_parameters():\n",
    "        #     print(name, param.grad.data.sum())\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        # print(\"\\nAfter zerograd\\n\")\n",
    "        # for name, param in self.policy_net.named_parameters():\n",
    "        #     print(name, param.grad.data.sum())\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = gym.make('CartPole-v0')\n",
    "    policy_net = PolicyNet(env.observation_space.shape, env.action_space.n).to(torch.device('cuda'))\n",
    "    base_net = Baseline(env.observation_space.shape).to(torch.device('cuda'))\n",
    "    # policy_net.load_state_dict(torch.load('./policynet'))\n",
    "    # base_net.load_state_dict(torch.load('./basenet'))\n",
    "    agent = Agent(policy_net, base_net)\n",
    "    rews = agent.train(env, 32, 200, 0.99, 5)\n",
    "    plt.plot(rews)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
