{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Double Deep Q-Learning (Double DQN)</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Q(s,a;\\theta) = r + \\gamma Q(s', argmax_{a'} Q(s', a'; \\theta); \\theta') \\tag{6}$$\n",
    "The main neural network, $\\theta$ determines the best next action $a'$, while the target network is used to evaluate this action and compute its Q-value. This simple change has been shown to reduce overestimations of Q-values in DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prelims "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "GAMMA = 0.99\n",
    "MINI_BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 50000\n",
    "MIN_REPLAY_SIZE = 1000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.02\n",
    "EPSILON_DECAY = 10000\n",
    "TARGET_UPDATE_FREQUENCY = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dqn import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_step(\n",
    "    env: gym.Env,\n",
    "    observation: np.ndarray,\n",
    "    online_net: DQN,\n",
    "    target_net: DQN,\n",
    "    epsilon_gen,\n",
    "    optimizer,\n",
    "    replay_buffer: deque,\n",
    "    reward_buffer: deque,\n",
    "    episode_reward: float,\n",
    "    criterion,\n",
    "    batch_size: int,\n",
    "):\n",
    "    epsilon = next(epsilon_gen)\n",
    "    random_sample = random.random()\n",
    "    action = (\n",
    "        env.action_space.sample()\n",
    "        if random_sample <= epsilon\n",
    "        else online_net.act(observation)\n",
    "    )\n",
    "\n",
    "    next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "    replay_buffer.append((observation, action, reward, terminated, next_observation))\n",
    "    observation = next_observation\n",
    "\n",
    "    episode_reward += reward\n",
    "\n",
    "    if terminated:\n",
    "        observation, info = env.reset()\n",
    "        reward_buffer.append(episode_reward)\n",
    "        episode_reward = 0.0\n",
    "\n",
    "    # Start Gradient Step\n",
    "    memories = random.sample(replay_buffer, batch_size)\n",
    "    all_obs_tensor = torch.as_tensor(np.asarray([memo_i[0] for memo_i in memories]), dtype=torch.float32)  # [batch, 4]\n",
    "    all_a_tensor = torch.as_tensor(np.asarray([memo_i[1] for memo_i in memories]), dtype=torch.int64).unsqueeze(-1)  # [batch, 1]\n",
    "    all_r_tensor = torch.as_tensor(np.asarray([memo_i[2] for memo_i in memories]), dtype=torch.float32).unsqueeze(-1)  # [batch, 1]\n",
    "    all_done_tensor = torch.as_tensor(np.asarray([memo_i[3] for memo_i in memories]), dtype=torch.float32).unsqueeze(-1)  # [batch, 1]\n",
    "    all_next_obs_tensor = torch.as_tensor(np.asarray([memo_i[4] for memo_i in memories]), dtype=torch.float32)  # [batch, 4]\n",
    "\n",
    "    # Compute Targets\n",
    "    target_q_inner = online_net(all_next_obs_tensor)\n",
    "    target_a = torch.argmax(target_q_inner, dim=1, keepdim=True)\n",
    "    target_q_values = target_net(all_next_obs_tensor)\n",
    "    target_a_q_values = torch.gather(input=target_q_values, dim=1, index=target_a)\n",
    "    targets = all_r_tensor + GAMMA * (1 - all_done_tensor) * target_a_q_values\n",
    "\n",
    "    # Compute Loss\n",
    "    q_values = online_net(all_obs_tensor)\n",
    "    a_q_values = torch.gather(input=q_values, dim=1, index=all_a_tensor)\n",
    "    loss = criterion(a_q_values, targets)\n",
    "\n",
    "    # Gradient Descent\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return observation, episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_replay_buffer(replay_buffer, env: gym.Env, replay: int):\n",
    "    observation, info = env.reset()\n",
    "    for _ in range(replay):\n",
    "        action = env.action_space.sample()\n",
    "        observation_, reward, terminated, truncated, info = env.step(action)\n",
    "        memory = (observation, action, reward, terminated, observation_)\n",
    "        replay_buffer.append(memory)\n",
    "        observation = observation_\n",
    "\n",
    "        if terminated:\n",
    "            observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_generator(start, end, decay):\n",
    "    step = 0\n",
    "    while True:\n",
    "        epsilon = np.interp(step, [0, decay], [start, end])\n",
    "        yield epsilon\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step: 0\n",
      "Avg reward: 0.0\n",
      "\n",
      "Step: 1000\n",
      "Avg reward: 20.224489795918366\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "reward_buffer = deque([0.0], maxlen=100)\n",
    "episode_reward = 0.0\n",
    "\n",
    "in_features = int(np.prod(env.observation_space.shape))\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "online_net = DQN(in_features, num_actions)\n",
    "target_net = DQN(in_features, num_actions)\n",
    "optimizer = torch.optim.Adam(online_net.parameters(), lr=5e-4)\n",
    "criterion = nn.functional.smooth_l1_loss\n",
    "\n",
    "create_replay_buffer(replay_buffer, env, MIN_REPLAY_SIZE)\n",
    "epsilon_gen = epsilon_generator(EPSILON_START, EPSILON_END, EPSILON_DECAY)\n",
    "observation, info = env.reset()\n",
    "\n",
    "for step in range(2000):\n",
    "    online_net.train()\n",
    "    observation, episode_reward = run_step(\n",
    "        env=env,\n",
    "        observation=observation,\n",
    "        online_net=online_net,\n",
    "        target_net=target_net,\n",
    "        epsilon_gen=epsilon_gen,\n",
    "        optimizer=optimizer,\n",
    "        replay_buffer=replay_buffer,\n",
    "        reward_buffer=reward_buffer,\n",
    "        episode_reward=episode_reward,\n",
    "        criterion=criterion,\n",
    "        batch_size=MINI_BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    # # After solved, watch it play\n",
    "    # if len(reward_buffer) >= 100:\n",
    "    #     if np.mean(reward_buffer) >= 30000:\n",
    "    #         while True:\n",
    "    #             action = online_net.act(observation)\n",
    "    #             observation, reward, terminated, truncated, info = env.step(action)\n",
    "    #             env.render()\n",
    "\n",
    "    #             if terminated:\n",
    "    #                 env.reset()\n",
    "\n",
    "    # Update target network\n",
    "    if step % TARGET_UPDATE_FREQUENCY == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "    # Print the training progress\n",
    "    if step % 1000 == 0:\n",
    "        print()\n",
    "        print(\"Step: {}\".format(step))\n",
    "        print(\"Avg reward: {}\".format(np.mean(reward_buffer)))\n",
    "        torch.save(online_net.state_dict(), \"./checkpoint/double_dqn_model_{}.pth\".format(step))\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
