{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Policy-Based Reinformance Learning</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Concepts\n",
    "\n",
    "## Reward Functions\n",
    "\n",
    "### Finite-horizon undiscounted return\n",
    "It sums up the rewards from a set of actions.\n",
    "$$ R(\\tau) = \\sum_{t=0}^{T} r_t $$\n",
    "\n",
    "### Infinite-horizon discounted return\n",
    "It has a new coefficient referred to as the discount factor. The discount factor corresponds to how far in the future those rewards were collected and is a number between zero and one.\n",
    "$$ R(\\tau) = \\sum_{t=0}^{\\infty} \\gamma^t r_t $$\n",
    "\n",
    "## Value Functions\n",
    "$$ V^{\\pi}(s) = E_{\\tau \\sim \\pi} [R(\\tau) | s_0 = s] $$\n",
    "$$ Q^{\\pi}(s, a) = E_{\\tau \\sim \\pi} [R(\\tau) | s_0 = s, a_0 = a] $$\n",
    "\n",
    "## Advantange Function\n",
    "$$ A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Policy Gradient Algorithm\n",
    "[For detail](https://medium.com/analytics-vidhya/a-deep-dive-into-vanilla-policy-gradients-3a79a95f3334)\n",
    "[zhihu](https://zhuanlan.zhihu.com/p/599897265)\n",
    "\n",
    "## Reward-to-go Method\n",
    "Only rewards after action can have an effect on the future actions, which is called the reward-to-go:\n",
    "$$ \\hat R_t = \\sum_{t'=t}^{T} R(s_{t'},a_{t'},s_{t'+1}) $$ \n",
    "\n",
    "$$ \\nabla J(\\pi_\\theta) = E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla \\theta \\log \\pi_\\theta(a_t|s_t) \\sum_{t'=t}^{T} R(s_{t'},a_{t'},s_{t'+1}) \\right] $$\n",
    "\n",
    "## Advantage Function\n",
    "The gradient of advantage function is:\n",
    "$$ \\nabla J(\\pi_\\theta) = E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla \\theta \\log \\pi_\\theta(a_t|s_t) A_\\pi(s_t, a_t) \\right]$$\n",
    "\n",
    "The value function $V_\\pi(s_t)$ is estimated by an approximate value function $V_{\\phi}(s_t)$, which is updated by:\n",
    "$$ \\phi_k = argmin_{\\phi} E_{s_t, \\hat R_t \\sim \\pi_k} [(V_{\\phi}(s_t) - \\hat R(t))^2] $$\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/vpg_algo.webp\" alt=\"example\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementation.vpg import VPGAgent\n",
    "from implementation.util import plot_graph\n",
    "\n",
    "def main():   \n",
    "    agent = VPGAgent('CartPole-v0', gamma=0.99)\n",
    "    rewards = agent.train(iterations=200, num_traj=32)\n",
    "    plot_graph(rewards)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO)\n",
    "\n",
    "## PPO with Adaptive KL Penalty\n",
    "Proximal policy optimization (PPO) is an algorithm that aims to address the overhead issue of TRPO by incorporating the constraint into the objective function as a penalty:\n",
    "$$ E_{\\tau \\sim \\pi_\\theta} \\left[ \\frac{\\pi \\theta(a_t|st)}{\\pi \\theta_{old}(a_t|st)} \\hat{A}t \\right] - C \\cdot \\overline{KL}\\pi \\theta_{old} (\\pi \\theta) \\tag{14}$$\n",
    "\n",
    "One challenge of PPO is choosing the appropriate value for C. To address this, the algorithm updates C based on the magnitude of the KL divergence. If the KL divergence is too high, C is increased, and, if it is too low, C is decreased. This allows for effective optimization over the course of training.\n",
    "\n",
    "## PPO with Clipped Objective\n",
    "Importance sampling can cause the variance of the sample to increase with the difference between the new and old strategies. We can directly limit the difference between the new and old strategies by limiting the difference in output action probabilities.\n",
    "\n",
    "$$ L^{CLIP}(\\theta) = E_{t} \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right] \\tag{15}$$\n",
    "\n",
    "$$ r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} \\tag{16}$$\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/ppo_algo.jpg\" alt=\"example\">\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/ppo_algo.svg\" alt=\"example\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Advantage Estimate (GAE)\n",
    "[For Detail](https://zhuanlan.zhihu.com/p/658971618)\n",
    "\n",
    "### Policy Gradiant Estimate\n",
    "$$ A^{\\pi}(a_t, s_t) := Q^{\\pi}(a_t, s_t) - V^{\\pi}(s_t) $$\n",
    "$$ V^{\\pi}(s_t) := E_{s_{t+1:\\infty}, a_{t:\\infty}} [\\sum_{l=0}^\\infty r_{t+l}] $$\n",
    "$$ Q^{\\pi}(s_t, a_t) := E_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\sum_{l=0}^\\infty r_{t+l}] $$\n",
    "\n",
    "### $\\gamma$-just\n",
    "$$ A^{\\pi,\\gamma}(a_t, s_t) := Q^{\\pi,\\gamma}(a_t, s_t) - V^{\\pi,\\gamma}(s_t) $$\n",
    "$$ V^{\\pi,\\gamma}(s_t) := E_{s_{t+1:\\infty}, a_{t:\\infty}} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}] $$\n",
    "$$ Q^{\\pi,\\gamma}(s_t, a_t) := E_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}] $$\n",
    "\n",
    "### GAE\n",
    "TD residual error:\n",
    "$$ \\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t) $$\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/gae.webp\" alt=\"example\">\n",
    "</center>\n",
    "\n",
    "$$ \\hat A_t ^{GAE(\\gamma, \\lambda)} = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION: 1 AVG REWARD: 15.0\n",
      "ITERATION: 11 AVG REWARD: 14.0\n",
      "ITERATION: 21 AVG REWARD: 25.0\n",
      "ITERATION: 31 AVG REWARD: 12.0\n",
      "ITERATION: 41 AVG REWARD: 13.0\n",
      "ITERATION: 51 AVG REWARD: 50.0\n",
      "ITERATION: 61 AVG REWARD: 36.0\n",
      "ITERATION: 71 AVG REWARD: 26.0\n",
      "ITERATION: 81 AVG REWARD: 10.0\n",
      "ITERATION: 91 AVG REWARD: 9.0\n",
      "ITERATION: 101 AVG REWARD: 10.0\n",
      "ITERATION: 111 AVG REWARD: 9.0\n",
      "ITERATION: 121 AVG REWARD: 9.0\n",
      "ITERATION: 131 AVG REWARD: 10.0\n",
      "ITERATION: 141 AVG REWARD: 10.0\n",
      "ITERATION: 151 AVG REWARD: 9.0\n",
      "ITERATION: 161 AVG REWARD: 10.0\n",
      "ITERATION: 171 AVG REWARD: 9.0\n",
      "ITERATION: 181 AVG REWARD: 10.0\n",
      "ITERATION: 191 AVG REWARD: 10.0\n",
      "ITERATION: 201 AVG REWARD: 10.0\n",
      "ITERATION: 211 AVG REWARD: 10.0\n",
      "ITERATION: 221 AVG REWARD: 12.0\n",
      "ITERATION: 231 AVG REWARD: 10.0\n",
      "ITERATION: 241 AVG REWARD: 9.0\n",
      "ITERATION: 251 AVG REWARD: 10.0\n",
      "ITERATION: 261 AVG REWARD: 10.0\n",
      "ITERATION: 271 AVG REWARD: 9.0\n",
      "ITERATION: 281 AVG REWARD: 10.0\n",
      "ITERATION: 291 AVG REWARD: 33.0\n",
      "ITERATION: 301 AVG REWARD: 11.0\n",
      "ITERATION: 311 AVG REWARD: 10.0\n",
      "ITERATION: 321 AVG REWARD: 24.0\n",
      "ITERATION: 331 AVG REWARD: 16.0\n",
      "ITERATION: 341 AVG REWARD: 28.0\n",
      "ITERATION: 351 AVG REWARD: 20.0\n",
      "ITERATION: 361 AVG REWARD: 19.0\n",
      "ITERATION: 371 AVG REWARD: 43.0\n",
      "ITERATION: 381 AVG REWARD: 32.0\n",
      "ITERATION: 391 AVG REWARD: 43.0\n",
      "ITERATION: 401 AVG REWARD: 29.0\n",
      "ITERATION: 411 AVG REWARD: 41.0\n",
      "ITERATION: 421 AVG REWARD: 13.0\n",
      "ITERATION: 431 AVG REWARD: 25.0\n",
      "ITERATION: 441 AVG REWARD: 31.0\n",
      "ITERATION: 451 AVG REWARD: 164.0\n",
      "ITERATION: 461 AVG REWARD: 102.0\n",
      "ITERATION: 471 AVG REWARD: 51.0\n",
      "ITERATION: 481 AVG REWARD: 24.0\n",
      "ITERATION: 491 AVG REWARD: 32.0\n",
      "ITERATION: 501 AVG REWARD: 79.0\n",
      "ITERATION: 511 AVG REWARD: 42.0\n",
      "ITERATION: 521 AVG REWARD: 120.0\n",
      "ITERATION: 531 AVG REWARD: 202.0\n",
      "ITERATION: 541 AVG REWARD: 989.0\n",
      "ITERATION: 551 AVG REWARD: 542.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtrain(iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, num_traj\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# plot_graph(rewards)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():   \n\u001b[1;32m      5\u001b[0m     agent \u001b[38;5;241m=\u001b[39m PPOAgent(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m'\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.98\u001b[39m, lam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_traj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rl-with-example/implementation/ppo.py:53\u001b[0m, in \u001b[0;36mPPOAgent.train\u001b[0;34m(self, iterations, num_traj)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mITERATION:\u001b[39m\u001b[38;5;124m\"\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAVG REWARD:\u001b[39m\u001b[38;5;124m\"\u001b[39m, iter_reward\u001b[38;5;241m/\u001b[39mnum_traj)\n\u001b[1;32m     52\u001b[0m     iter_rewards\u001b[38;5;241m.\u001b[39mappend(iter_reward\u001b[38;5;241m/\u001b[39mnum_traj)\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_records\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m iter_rewards\n",
      "File \u001b[0;32m~/rl-with-example/implementation/ppo.py:83\u001b[0m, in \u001b[0;36mPPOAgent.update_network\u001b[0;34m(self, batch_records)\u001b[0m\n\u001b[1;32m     81\u001b[0m ep_actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(ep_actions)\n\u001b[1;32m     82\u001b[0m ep_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(ep_states)\n\u001b[0;32m---> 83\u001b[0m ep_advantages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_advantages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mep_rewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mep_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscount_rewards(ep_rewards)\n\u001b[1;32m     86\u001b[0m value_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(torch\u001b[38;5;241m.\u001b[39mcat(ep_values), torch\u001b[38;5;241m.\u001b[39mtensor(targets))\n",
      "File \u001b[0;32m~/rl-with-example/implementation/ppo.py:70\u001b[0m, in \u001b[0;36mPPOAgent.calculate_advantages\u001b[0;34m(self, rewards, values)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(rewards) \u001b[38;5;241m-\u001b[39m t \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     69\u001b[0m     delta \u001b[38;5;241m=\u001b[39m rewards[t\u001b[38;5;241m+\u001b[39ml] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m*\u001b[39mvalues[t\u001b[38;5;241m+\u001b[39ml\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m values[t\u001b[38;5;241m+\u001b[39ml]\n\u001b[0;32m---> 70\u001b[0m     ad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlam)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39ml)\u001b[38;5;241m*\u001b[39m(delta)\n\u001b[1;32m     71\u001b[0m ad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlam)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39ml)\u001b[38;5;241m*\u001b[39m(rewards[t\u001b[38;5;241m+\u001b[39ml] \u001b[38;5;241m-\u001b[39m values[t\u001b[38;5;241m+\u001b[39ml])\n\u001b[1;32m     72\u001b[0m advantages[t] \u001b[38;5;241m=\u001b[39m ad\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from implementation.ppo import PPOAgent\n",
    "from implementation.util import plot_graph\n",
    "\n",
    "def main():   \n",
    "    agent = PPOAgent('CartPole-v1', gamma=0.98, lam=1, epsilon=0.2)\n",
    "    rewards = agent.train(iterations=1000, num_traj=1)\n",
    "    # plot_graph(rewards)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
