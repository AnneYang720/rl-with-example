{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Policy-Based Reinformance Learning</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Concepts\n",
    "\n",
    "## Reward Functions\n",
    "\n",
    "### Finite-horizon undiscounted return\n",
    "It sums up the rewards from a set of actions.\n",
    "$$ R(\\tau) = \\sum_{t=0}^{T} r_t $$\n",
    "\n",
    "### Infinite-horizon discounted return\n",
    "It has a new coefficient referred to as the discount factor. The discount factor corresponds to how far in the future those rewards were collected and is a number between zero and one.\n",
    "$$ R(\\tau) = \\sum_{t=0}^{\\infty} \\gamma^t r_t $$\n",
    "\n",
    "## Value Functions\n",
    "$$ V^{\\pi}(s) = E_{\\tau \\sim \\pi} [R(\\tau) | s_0 = s] $$\n",
    "$$ Q^{\\pi}(s, a) = E_{\\tau \\sim \\pi} [R(\\tau) | s_0 = s, a_0 = a] $$\n",
    "\n",
    "## Advantange Function\n",
    "$$ A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Policy Gradient Algorithm\n",
    "[For detail](https://medium.com/analytics-vidhya/a-deep-dive-into-vanilla-policy-gradients-3a79a95f3334)\n",
    "[zhihu](https://zhuanlan.zhihu.com/p/599897265)\n",
    "\n",
    "## Reward-to-go Method\n",
    "Only rewards after action can have an effect on the future actions, which is called the reward-to-go:\n",
    "$$ \\hat R_t = \\sum_{t'=t}^{T} R(s_{t'},a_{t'},s_{t'+1}) $$ \n",
    "\n",
    "$$ \\nabla J(\\pi_\\theta) = E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla \\theta \\log \\pi_\\theta(a_t|s_t) \\sum_{t'=t}^{T} R(s_{t'},a_{t'},s_{t'+1}) \\right] $$\n",
    "\n",
    "## Advantage Function\n",
    "The gradient of advantage function is:\n",
    "$$ \\nabla J(\\pi_\\theta) = E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla \\theta \\log \\pi_\\theta(a_t|s_t) A_\\pi(s_t, a_t) \\right]$$\n",
    "\n",
    "The value function $V_\\pi(s_t)$ is estimated by an approximate value function $V_{\\phi}(s_t)$, which is updated by:\n",
    "$$ \\phi_k = argmin_{\\phi} E_{s_t, \\hat R_t \\sim \\pi_k} [(V_{\\phi}(s_t) - \\hat R(t))^2] $$\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/vpg_algo.webp\" alt=\"example\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementation.vpg import VPGAgent\n",
    "from implementation.util import plot_graph\n",
    "\n",
    "def main():   \n",
    "    agent = VPGAgent('CartPole-v0', gamma=0.99)\n",
    "    rewards = agent.train(iterations=200, num_traj=32)\n",
    "    plot_graph(rewards)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO)\n",
    "\n",
    "## PPO with Adaptive KL Penalty\n",
    "Proximal policy optimization (PPO) is an algorithm that aims to address the overhead issue of TRPO by incorporating the constraint into the objective function as a penalty:\n",
    "$$ E_{\\tau \\sim \\pi_\\theta} \\left[ \\frac{\\pi \\theta(a_t|st)}{\\pi \\theta_{old}(a_t|st)} \\hat{A}t \\right] - C \\cdot \\overline{KL}\\pi \\theta_{old} (\\pi \\theta) \\tag{14}$$\n",
    "\n",
    "One challenge of PPO is choosing the appropriate value for C. To address this, the algorithm updates C based on the magnitude of the KL divergence. If the KL divergence is too high, C is increased, and, if it is too low, C is decreased. This allows for effective optimization over the course of training.\n",
    "\n",
    "## PPO with Clipped Objective\n",
    "Importance sampling can cause the variance of the sample to increase with the difference between the new and old strategies. We can directly limit the difference between the new and old strategies by limiting the difference in output action probabilities.\n",
    "\n",
    "$$ L^{CLIP}(\\theta) = E_{t} \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right] \\tag{15}$$\n",
    "\n",
    "$$ r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} \\tag{16}$$\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/ppo_algo.jpg\" alt=\"example\">\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/ppo_algo.svg\" alt=\"example\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Advantage Estimate (GAE)\n",
    "[For Detail](https://zhuanlan.zhihu.com/p/658971618)\n",
    "\n",
    "### Policy Gradiant Estimate\n",
    "$$ A^{\\pi}(a_t, s_t) := Q^{\\pi}(a_t, s_t) - V^{\\pi}(s_t) $$\n",
    "$$ V^{\\pi}(s_t) := E_{s_{t+1:\\infty}, a_{t:\\infty}} [\\sum_{l=0}^\\infty r_{t+l}] $$\n",
    "$$ Q^{\\pi}(s_t, a_t) := E_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\sum_{l=0}^\\infty r_{t+l}] $$\n",
    "\n",
    "### $\\gamma$-just\n",
    "$$ A^{\\pi,\\gamma}(a_t, s_t) := Q^{\\pi,\\gamma}(a_t, s_t) - V^{\\pi,\\gamma}(s_t) $$\n",
    "$$ V^{\\pi,\\gamma}(s_t) := E_{s_{t+1:\\infty}, a_{t:\\infty}} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}] $$\n",
    "$$ Q^{\\pi,\\gamma}(s_t, a_t) := E_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}] $$\n",
    "\n",
    "### GAE\n",
    "TD residual error:\n",
    "$$ \\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t) $$\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/gae.webp\" alt=\"example\">\n",
    "</center>\n",
    "\n",
    "$$ \\hat A_t ^{GAE(\\gamma, \\lambda)} = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementation.ppo import PPOAgent\n",
    "from implementation.util import plot_graph\n",
    "\n",
    "def main():   \n",
    "    agent = PPOAgent('CartPole-v1', gamma=0.98, lam=1, epsilon=0.2)\n",
    "    rewards = agent.train(iterations=1000, num_traj=1)\n",
    "    # plot_graph(rewards)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
