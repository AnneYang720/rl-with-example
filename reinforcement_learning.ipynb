{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning With Practice\n",
    "\n",
    "I'm writing this notebook for a better understanding of Reinforcement Learning and its practical implementation. I got the motive while reading [A Survey on Deep Reinforcement Learning Algorithms for Robotic Manipulation](https://www.mdpi.com/1424-8220/23/7/3762). I found it obscure to apply the algorithms to real-world problems, like I don't know what's the reward or policy.\n",
    "\n",
    "I introduce all the algorithms briefly and use them to solve the [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/) environment from OpenAI Gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Notations**\n",
    "\n",
    "A Markov devision process (MDP) is defined as a tuple $\\langle S, A, r, T, \\gamma \\rangle$ where:\n",
    "- $S$ - current state\n",
    "- $A$ - action\n",
    "- $r$ - reward for taking action $A$ in state $S$\n",
    "- $T$ - state transition function\n",
    "- $\\gamma$ - discount factor implying that a reward obtained in the future is worth a smaller amount than an immediate reward\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About CartPole\n",
    "1. action space: a ndarray with shape `(1,)` which can take values `{0, 1}` indicating the direction of the fixed force the cart is pushed with.\n",
    "2. observation space: a ndarray with shape `(4,)` containing the cart position, cart velocity, pole angle, and pole velocity at the tip.\n",
    "3. reward: `1` for every step taken, including the termination step.\n",
    "4. termination: the episode ends when the pole is more than 15 degrees from vertical or the cart moves more than 2.4 units from the center.\n",
    "5. truncation: the episode ends after 500 steps for v1 (200 for v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Value-Based RL\n",
    "\n",
    "### 1.1 Q-Learning\n",
    "$$ Q(s, a) = r(s, a) + \\gamma \\max_{a} Q(s', a) \\tag{1} $$\n",
    "\n",
    "$Q(s, a)$ is the Bellman action-value function, which estimates how good it is to take an action at a given state. Q-Learning is off-policy that learns the optimal policy directly.\n",
    "\n",
    "### 1.2 SARSA\n",
    "$$ Q(s, a) = Q(s, a) + \\alpha \\left[ R + \\gamma Q(s', a') - Q(s, a) \\right] \\tag{2}$$\n",
    "SARSA is on-policy, which means $a'$ in $Q(s', a')$ follows the current policy.\n",
    "\n",
    "### 1.3 Deep Q-Learning (DQN)\n",
    "[Deep Q-Learning](dqn.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([3, 4], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_buffer = deque(maxlen=2)\n",
    "\n",
    "\n",
    "def creat(buffer):\n",
    "    for i in range(5):\n",
    "        buffer.append(i)\n",
    "\n",
    "\n",
    "creat(replay_buffer)\n",
    "print(replay_buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
